
---

# vLLM 学习计划（基于与ChatGPT对话的总结）

## 📋 学习目标与定位

### 核心目标
1. **短期目标**（3-6个月）：深入理解vLLM源码，掌握LLM Serving System的核心设计
2. **中期目标**（6-12个月）：成为分布式LLM推理系统专家，能够设计和优化调度策略
3. **职业目标**：Microsoft AI Foundry Scheduler

### 关键认知
- **vLLM ≠ LLM模型**：vLLM是推理执行引擎/系统，模型只是它加载执行的"程序/数据"
- **vLLM的核心价值**：
  - Continuous Batching（连续批处理）
  - PagedAttention（KV Cache虚拟化）
  - Token-level Scheduling（令牌级调度）
  - Resource Arbitration（资源仲裁）
- **学习策略**：以Scheduler为主线（40%精力），Runtime Feature为核心（40%精力），Integration为辅（20%精力）

## 🎯 Phase 0：环境准备与心智模型建立（第1周）

### 环境配置
```bash
# 1. 创建Conda环境
conda create -n vllm python=3.10 -y
conda activate vllm

# 2. 安装PyTorch（带CUDA）
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# 3. 验证GPU
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"

# 4. 克隆并安装vLLM源码（editable模式）
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

### 建立心智模型（必须先做）
**回答这些问题**：
- [ ] vLLM的"单位工作量"是什么？（答案：token）
- [ ] 核心瓶颈在哪里？（答案：KV cache）
- [ ] Scheduler在什么时候做决策？（答案：每个decode step）
- [ ] vLLM vs PyTorch Distributed的区别？（vLLM在PyTorch之上，重新设计了执行模型、内存管理和调度策略）

### 最小可运行验证
```python
from vllm import LLM, SamplingParams

# 使用小模型快速验证
llm = LLM(
    model="facebook/opt-125m",
    max_model_len=128,
)

params = SamplingParams(max_tokens=16)
outputs = llm.generate(["Hello, my name is"], params)
print(outputs[0].outputs[0].text)
```

### 追踪请求流
- [ ] 设置断点在`LLM.generate()`
- [ ] 跟踪：HTTP → Scheduler → Engine → GPU
- [ ] 理解：Prefill vs Decode的区别

**输出物**：一页"vLLM系统架构图"，标注关键模块的职责

---

## 🚀 Phase 1：Feature #1 - Continuous Batching（第2-3周）

### 为什么从这里开始？
- vLLM存在的根本理由
- 定义了Scheduler的"时间观"
- 所有其他feature的基础

### 学习目标
理解为什么batch不是"请求集合"而是"token step的流"

### 核心问题
**背景**：
- 请求长度不同
- 请求持续到来
- GPU不该idle

**关键设计**：
- batch ≠ request
- batch = token step的集合
- Scheduler每step重新构建batch

### 源码路径与阅读顺序
```
vllm/
├── engine/
│   ├── llm_engine.py          # 真正的同步逻辑
│   └── async_llm_engine.py    # 异步wrapper
├── core/
│   └── scheduler.py            # ⭐ 核心调度逻辑
├── sequence.py                 # Sequence状态管理
└── worker/
    └── worker.py               # GPU worker抽象
```

### 关键代码点
1. **Scheduler.schedule()** - 每个step的调度决策
2. **SequenceGroup** - 请求如何组织
3. **SequenceStatus** - RUNNING/WAITING/FINISHED等状态

### 必须理解的5件事
1. [ ] Batch是如何"增长"和"收缩"的
2. [ ] 一个Sequence的完整生命周期
3. [ ] Prefill和Decode如何交错执行
4. [ ] Scheduler在哪些点必须介入
5. [ ] KV cache如何被多个sequence共享时间窗口

### 实践任务
```python
# 任务1：打印每step的active sequence数量
# 在scheduler.py中添加日志

# 任务2：运行多个不同长度的请求，观察batch变化
prompts = [
    "Short",  # 短请求
    "This is a much longer prompt " * 10,  # 长请求
    "Medium length prompt"  # 中等请求
]

# 任务3：思考题
# 如果fairness是第一目标而不是throughput，scheduler应该怎么改？
```

### 自检问题（必须全部回答）
- [ ] 为什么decode阶段是token-level scheduling？
- [ ] 为什么batch size在vLLM里不是参数，而是状态？
- [ ] 为什么tail latency被显著改善？
- [ ] 如果禁用Continuous Batching，vLLM还剩什么？
- [ ] 如果我是Foundry，如何把这个能力转成SLA？

**输出物**：
1. 一份"Continuous Batching执行时间线图"
2. Scheduler决策点标注
3. 与Kafka的对比笔记（Consumer group rebalance vs Continuous batching）

---

## 🧩 Phase 2：Feature #2 - PagedAttention & KV Cache Management（第4-5周）

### 核心问题
**背景**：
- KV cache占90%显存
- 不同request生命周期不同
- 传统allocator导致碎片化

**关键设计**：
- KV cache ≠ tensor
- KV cache = paged memory（像OS的page cache）
- Block-level管理

### 对Scheduler的影响
Scheduler必须：
- 感知KV cache使用情况
- 参与admission control（能否接受新请求）
- 决定eviction策略（驱逐哪些block）

### 源码路径
```
vllm/
├── core/
│   └── block_manager.py       # ⭐ KV cache管理核心
├── attention/
│   └── backends/
│       └── flash_attn.py      # Attention实现
└── worker/
    └── cache_engine.py        # Cache实际存储
```

### 关键概念
- **Physical Block**：GPU上实际的内存block
- **Logical Block**：Sequence视角的虚拟block
- **Block Table**：逻辑到物理的映射
- **COW (Copy-on-Write)**：Prefix caching优化

### 实践任务
```python
# 任务1：监控block分配
# 打印每个step的block allocation/free

# 任务2：触发eviction
# 故意设置小的block数量，观察eviction行为

# 任务3：对比实验
# 关闭paging vs 开启paging的显存使用对比
```

### 必须理解的点
- [ ] 为什么不能直接用PyTorch allocator？
- [ ] Eviction发生在什么时候？谁触发？
- [ ] Block size如何选择？(trade-off分析)
- [ ] Prefix caching如何实现？

### 与系统知识的映射
| OS概念 | vLLM概念 |
|--------|----------|
| Virtual Memory | Logical Blocks |
| Physical Memory | Physical Blocks |
| Page Table | Block Table |
| Page Fault | Block Allocation |
| Eviction | Preemption |

**输出物**：
1. KV Cache生命周期图
2. Block Manager设计文档
3. 与Kafka Log Segment的类比分析

---

## ⚡ Phase 3：Feature #3 - Chunked Prefill（第6周）

### 核心问题
**背景**：
- Prefill长且慢（O(n²)复杂度）
- Decode短但多（O(n)复杂度）
- Prefill阻塞decode → tail latency爆炸

**关键设计**：
- Prefill被切成chunk
- Prefill和decode交错执行
- 牺牲少量TTFT（Time To First Token）换取整体throughput

### Scheduler的新增能力
1. **两种phase**：
   - PREFILL
   - DECODE
2. **Chunk size决策**（配置vs动态）
3. **Priority queue**（prefill vs decode优先级）

### 源码路径
```
vllm/core/scheduler.py
  - _schedule_prefills()
  - _schedule_running()
  - _schedule_swapped()
```

### 关键参数
- `max_num_batched_tokens`：单step最大token数
- `max_num_seqs`：单step最大sequence数
- chunk边界如何决定

### 实践任务
```python
# 任务1：改变chunk size
llm = LLM(
    model="facebook/opt-125m",
    max_num_batched_tokens=256,  # 默认是2048，尝试不同值
)

# 任务2：观察指标
# - TTFT (Time To First Token)
# - Throughput (tokens/sec)
# - Tail latency

# 任务3：极端测试
# 一个超长prefill + 多个decode请求同时到达
```

### 自检问题
- [ ] Chunk boundary如何决定？
- [ ] 为什么不能把prefill全部做完？
- [ ] Prefill和decode如何混batch？（技术约束）
- [ ] 对tail latency的量化影响？

**输出物**：
1. Chunked Prefill vs Non-chunked性能对比报告
2. Scheduler决策流程图（prefill/decode混合调度）

---

## 🔀 Phase 4：Feature #4 - Prefill/Decode Disaggregation（第7周）

### 核心问题
**背景**：
- Prefill：compute-bound（计算密集）
- Decode：memory-bound（内存密集）
- 混在一起效率低

**关键设计**：
- 独立的Prefill worker
- 独立的Decode worker
- KV cache跨worker共享/传输

### Scheduler的变化
- 多worker pool管理
- 跨pool调度决策
- KV cache一致性保证

### 源码路径
```
vllm/
├── engine/
│   └── llm_engine.py
├── executor/
│   ├── gpu_executor.py
│   └── ray_gpu_executor.py    # 分布式执行
```

### 关键问题
- [ ] Request如何在prefill→decode worker间迁移？
- [ ] KV cache如何共享？（内存拷贝vs共享内存）
- [ ] Failure handling：worker挂了怎么办？

### 实践（可选，取决于时间）
需要Ray环境，可以先理解架构，暂不深入实现

**输出物**：
1. PD Disaggregation架构图
2. Request迁移路径流程图

---

## 🎛️ Phase 5：Scheduler Policy & Admission Control（第8周）

### 核心问题
**背景**：
- 资源有限（GPU memory）
- 请求不可控（burst traffic）
- 需要SLA保证

**关键设计**：
- Admission control：拒绝vs排队vs降级
- Scheduling policy：FCFS vs Priority vs Fair
- Backpressure机制

### 源码路径
```
vllm/core/
├── scheduler.py
├── policy.py          # ⭐ 调度策略
└── evictor.py         # Eviction策略
```

### 实践任务
```python
# 任务1：实现简单的memory-based admission control
def can_admit_request(self, seq_group):
    available_blocks = self.block_manager.get_num_free_blocks()
    required_blocks = estimate_blocks(seq_group)
    return available_blocks >= required_blocks + SAFETY_MARGIN

# 任务2：对比不同策略
# - FCFS (First Come First Serve)
# - Priority-based
# - Fair sharing

# 任务3：压力测试
# 故意发送超过容量的请求，观察系统行为
```

### 与Foundry的对齐
这一部分直接对应Foundry Scheduler的核心职责：
- **Multi-tenant**: 如何在多个tenant间公平分配
- **SLA**: 如何保证latency/throughput
- **Cost**: 如何优化GPU利用率

**输出物**：
1. Admission Control设计文档
2. 不同Policy的trade-off分析
3. 与Kafka quota机制的对比

---

## 🌐 Phase 6：Distributed & Multi-GPU（第9周）

### 核心问题
- 单卡放不下模型（参数太大）
- 需要并行推理提升吞吐

### 关键概念
- **Tensor Parallel (TP)**：单层内切分
- **Pipeline Parallel (PP)**：跨层切分
- **Communication**: NCCL, collective operations

### 源码路径
```
vllm/
├── distributed/
│   ├── parallel_state.py      # ⭐ 分布式状态管理
│   └── communication_op.py    # 通信原语
├── worker/
│   └── model_runner.py
└── model_executor/
    └── models/
        └── llama.py           # 看TP如何实现
```

### 关键理解
- [ ] TP如何切分attention heads？
- [ ] All-reduce在哪些点发生？
- [ ] PP的stage间通信
- [ ] 与Kafka partition的类比

**输出物**：
1. TP/PP通信模式图
2. 分布式调度vs单机调度的差异分析

---

## 🎓 Phase 7：对齐Foundry Scheduler（第10-12周）

### 从vLLM到Foundry的跃迁

#### vLLM覆盖的（单系统视角）
- Token-level scheduling
- KV cache管理
- Single-tenant优化

#### Foundry需要的（平台视角）
- **Multi-model scheduling**：一个GPU跑多个模型
- **Multi-tenant isolation**：租户隔离
- **SLA management**：不同优先级
- **Cost optimization**：$/token
- **Failure recovery**：容错与恢复
- **Elastic scaling**：动态扩缩容

### 实践：设计Foundry-style Scheduler

#### 思考题（面试/工作常见）
1. **如果一个GPU上跑3个模型？**
   - KV cache如何隔离？
   - Scheduler如何决策？

2. **如果某个tenant突然flood？**
   - Admission control策略？
   - 如何保护其他tenant？

3. **如果某GPU挂了？**
   - Request如何迁移？
   - KV cache如何恢复？

4. **如何做cost-aware scheduling？**
   - Spot instance vs On-demand
   - Preemption策略

#### 实践项目（强烈推荐）
设计一个简化版Foundry Scheduler：
```python
class FoundryScheduler:
    def __init__(self):
        self.backends = {
            'vllm': VLLMBackend(),
            'deepspeed': DeepSpeedBackend(),
        }
        self.tenants = {}
    
    def schedule_request(self, request, tenant_id):
        # 1. Admission control
        if not self.can_admit(request, tenant_id):
            return reject_or_queue(request)
        
        # 2. Backend selection
        backend = self.select_backend(request)
        
        # 3. Resource allocation
        resources = self.allocate_resources(request, tenant_id)
        
        # 4. Dispatch
        return backend.execute(request, resources)
```

**输出物**：
1. Foundry Scheduler设计文档
2. vLLM vs Foundry能力映射表
3. Multi-tenant调度模拟器（简化版）

---

## 📊 学习里程碑与时间安排

### 时间线（12周计划）
| 周数 | 阶段 | 关键输出 | 时间投入 |
|------|------|----------|----------|
| 1 | Phase 0 | 环境+心智模型 | 10小时 |
| 2-3 | Phase 1 | Continuous Batching | 15小时/周 |
| 4-5 | Phase 2 | PagedAttention | 15小时/周 |
| 6 | Phase 3 | Chunked Prefill | 12小时 |
| 7 | Phase 4 | PD Disaggregation | 12小时 |
| 8 | Phase 5 | Scheduler Policy | 15小时 |
| 9 | Phase 6 | Distributed | 10小时 |
| 10-12 | Phase 7 | Foundry对齐 | 20小时/周 |

**总计**: 约150-180小时，平均每周12-15小时

### 与你的实际情况对齐
- **7月底回国**：Phase 1-5完成后可以回国
- **回国期间**：可以做理论总结、写文档、读论文
- **回国后**：Phase 6-7（分布式+Foundry对齐）

---

## 🎯 与职业目标的对齐

### Foundry Scheduler面试准备
掌握这些话题后，你可以自信地讨论：
- ✅ Resource arbitration（资源仲裁）
- ✅ Token-level scheduling
- ✅ Multi-tenant isolation
- ✅ Admission control
- ✅ Latency vs throughput trade-off
- ✅ Cost optimization

### 与其他学习的平衡
- **Kafka源码**：继续保持，但"带问题读"（30%时间）
- **vLLM**：主线（50%时间）
- **论文/博客**：补充（20%时间）

---

## 📚 推荐资源

### 论文
- [ ] vLLM原始论文：Efficient Memory Management for Large Language Model Serving with PagedAttention
- [ ] FlashAttention论文
- [ ] Orca论文（vLLM的对比baseline）

### 代码仓库
- [ ] [vLLM GitHub](https://github.com/vllm-project/vllm)
- [ ] [Ray Serve](https://docs.ray.io/en/latest/serve/)（对比学习）

### 博客与文档
- [ ] vLLM官方博客
- [ ] Databricks的LLM Serving系列
- [ ] Anyscale的Ray Serve博客

---

## ✅ 学习检查清单

### 每个Phase完成后自检
- [ ] 我能用一页纸解释这个feature的核心设计吗？
- [ ] 我知道为什么要这样设计（而不是别的方式）吗？
- [ ] 我能指出这个设计的trade-off吗？
- [ ] 如果我是Foundry，我会如何使用/改进这个feature？

### 最终检验（12周后）
- [ ] 我能从零设计一个简化版vLLM吗？
- [ ] 我能解释vLLM vs DeepSpeed的本质区别吗？
- [ ] 我能讨论Foundry Scheduler的核心挑战吗？
- [ ] 我有至少3个可展示的项目/分析吗？

---

## 🚨 风险与应对

### 潜在风险
1. **时间不足**（转组+回国）
   - 应对：聚焦Phase 1-5，Phase 6-7可后置
   
2. **深度vs广度平衡**
   - 应对：每个feature先理解"为什么"，再看"怎么做"
   
3. **与Foundry转组时间冲突**
   - 应对：Phase 1-3完成即可支撑面试，不必等全部学完

### 学习节奏调整
- **前6周**：密集学习（Phase 1-3）
- **第7-9周**：巩固+准备回国
- **回国期间**：理论总结
- **回国后**：实战对齐（Phase 7）

---

## 💡 最重要的原则

### 不要追求"学完"
vLLM在快速演进，你永远"差一点就学完"。

### 目标是"能力迁移"
不是记住vLLM的每一行代码，而是：
- 理解LLM serving的系统问题
- 掌握scheduler的设计思维
- 能在Foundry场景下做trade-off

### 保持"专家视角"
每学一个feature，问自己：
- "如果没有这个feature，系统会在哪崩？"
- "如果我是Foundry，我会如何暴露这个能力？"

---

## 🎬 立即开始的第一步

**今天就做**：
1. ✅ 配置环境（2小时）
2. ✅ 跑通Hello World（30分钟）
3. ✅ 画出vLLM架构图（1小时）
4. ✅ 设置第一个断点，跟踪一次request（2小时）

**本周完成**：
- Phase 0全部内容
- Continuous Batching的初步理解

**记住**：
> "vLLM是通往Foundry scheduler的主干道，DeepSpeed是后续加深地基的工具。"
> 
> "不要问'这个feature怎么写'，要问'如果没有这个feature，系统会在哪崩'。"

---

祝学习顺利！每完成一个Phase，记得回顾这个计划，调整节奏。

**最后一句话**：
转组机会比"学完vLLM"更稀缺。如果Foundry窗口出现，Phase 1-3完成即可转，不要等"完美准备"。