* Source: https://www.youtube.com/watch?v=uclfcBc8hPE  

* Start with `vllm/examples/offline_inference/basic/basic.py`  
* Huggingface æ˜¯å„ç§LLMæ¨¡å‹çš„hub  

## vLLMä»£ç åº“
### æ¨¡å—
* FE: LLM, API Server
    * Path: `vllm/entrypoints/`
        * for offline inference: LLM: `./llm.py`
        * for online serving: API Server: `./openai/api_server.py`
            * FastAPI routes requets to FE functions
* BE: Engine æ˜¯vLLMçš„ä¸»è¦åŠŸèƒ½
    * Path: `vllm/engine/`
    * `./llm_engine.py` æ˜¯çœŸæ­£çš„(åŒæ­¥)é€»è¾‘æ‰€åœ¨
    * `./async_llm_engine.py` æ˜¯å¥—äº†ä¸€å±‚å¼‚æ­¥é€»è¾‘ï¼Œå¯çœ‹åšllm_engineçš„agent/proxy
* Core modules:
    * Path: `vllm/core/`
    * Scheduler: `./scheduler.py`
        * ä½œç”¨ï¼šæ¯ä¸ªstepä¸­å‡†å¤‡ä¸€ä¸ªrequest batchï¼Œæ”¾ä»€ä¹ˆrequestsæäº¤ç»™engineå‡ºå¤„ç†
            * step: ä¸€æ¬¡inferenceï¼Œå°±æ˜¯è¯´ä¸€ä¸ªå­—token
        * ç°æœ‰ç³»ç»Ÿorcaåœ¨æ‰“åŒ…çš„è¿‡ç¨‹ä¸­å¯¹GPUå†…å­˜çš„åˆ©ç”¨ç‡ä¸è¶³
            * é—®é¢˜ï¼špreallocate continuous memory
            * äºæ˜¯å¼•å…¥äº† kv cache manageræ¥å¯¹gpuå†…å­˜è¿›è¡Œæ›´ç»†ç²’åº¦çš„ç®¡ç†
    * KV cache manager: `./block_manager.py`
        * æ ¸å¿ƒ: Paged Attention
    * Evictor: `./evictor.py`
        * preempt æœºåˆ¶æ‰€åœ¨ï¼šrequestä½¿ç”¨GPUå†…å­˜çš„swapper
        * prefix/history caching: requesté—´å¤ç”¨ç›¸åŒçš„å‰ç¼€
            * what if prefix doesn't match: `CacheBlend`
            * what if prefix cache on another machine: `cache sharing across nodes`
        * KV cache optimization
            * DeepSeek (MLA: multi layer attention æµ“ç¼©ç‰ˆçš„attention)
* Worker
    * å„ç§ç¡¬ä»¶çš„æŠ½è±¡ï¼šGPU worker, CPU worker, TPU worker, XPU worker, HPU worker, etc.
    * Path: `vllm/worker/`
    * ä½œç”¨ï¼šä¸ºModelçš„è¿è¡Œåˆå§‹åŒ–ä¸€äº›åˆ—çš„ç¯å¢ƒï¼Œä¾‹å¦‚åˆ†å¸ƒå¼ç¯å¢ƒç­‰
    * WorkerBase: `worker_base.py`
        * ä½œç”¨ï¼šæ‰€æœ‰workerçš„åŸºæœ¬æŠ½è±¡
    * Worker: `worker.py`
        * ä½œç”¨ï¼šè¡ç”Ÿå‡ºçš„çœŸæ­£å¹²æ´»çš„ç‰›é©¬ç±»ï¼Œé»˜è®¤ä½¿ç”¨GPU
        * æœ€ç®€å•ï¼Œfocusåœ¨è¿™ä¸ªworkerè€Œä¸æ˜¯å…¶ä»–ç¡¬ä»¶workerï¼Œå¦åˆ™å„ç§å¥‡æ€ªçš„é—®é¢˜ã€‚ã€‚ã€‚

* Model Executor (Model Runner)
    * ä½œç”¨ï¼šæ¨¡å‹åŒ…äº†ä¸€å±‚å£³ï¼Œåº”è¯¥æ˜¯modelè¿è¡Œçš„runtime manager
    * Path: `vllm/model_executor/`
    * models: `./models/`
        * ä½œç”¨ï¼šå„ç§æ¨¡å‹çš„å®ç°
        * æœ€é‡è¦çš„æ¨¡å‹ï¼š`./llamma.py`
            * é‡Œé¢çš„265è¡Œçš„`forward()`å‡½æ•°æ˜¯ç»å¸¸éœ€è¦ç¢°åˆ°çš„å‡½æ•°
    * Modelling
        * ä½œç”¨ï¼šæŠŠHuggingfaceä¸Šé¢ï¼Œ`vllm/model_executor/models/` ä¸‹é¢ç­‰åƒå¥‡ç™¾æ€ªçš„æ¨¡å‹ï¼Œæ”¹é€ æˆvLLMå¯ä»¥executeçš„è¿‡ç¨‹

* Attention backend
    * ä½œç”¨ï¼šçœŸæ­£å®ç°äº†Attentionçš„åœ°æ–¹
    * Path: `/vllm/attention/backends/`
    * Flash attention: `flash_attn.py`
        * ä¼˜åŒ–softmaxï¼Œé‡‡ç”¨é€’å½’çš„æ–¹å¼è®¡ç®—ä¸¤ä¸¤tokenä¹‹é—´çš„å…³ç³»
        * å¦åˆ™nä¸ªtokenéœ€è¦è®¡ç®—n**2çš„å¤æ‚åº¦è®¡ç®—ä¸¤ä¸¤tokenä¹‹é—´çš„å…³ç³»ï¼Œéå¸¸å ç”¨å†…å­˜å’Œè®¡ç®—èµ„æº
        * prefillæ˜¯å¤„ç†è¾“å…¥input tokensï¼Œdecodeæ˜¯ç”Ÿæˆä¸€ä¸ªä¸€ä¸ªoutput token
        * 771è¡Œ kernel `flash_attn_varlen_func()` ä¸éœ€è¦ä»GPUæ‹¿ä»»ä½•data
        * 824è¡Œ kernel `flash_attn_with_kvcache()` ä»paged memoryæ‹¿dataæ¥è¿›è¡Œè®¡ç®—

### vllm
[LMCache](https://github.com/LMCache/LMCache/)
[vllm production statck](https://github.com/vllm-project/production-stack)
* vllmä¸Šk8s

### Feature
* Distributed communication/inference
    * Why distributed inference?
        * reason: 
            * ä¸€å¼ GPUå†…å­˜æ— æ³•æŠŠä¸€ä¸ªå¤§ä¸€ç‚¹çš„modelçš„parametersæ”¾ä¸‹ã€‚
            * ç°åœ¨åŒæ—¶è€ƒè™‘å……åˆ†åˆ©ç”¨è®¡ç®—èµ„æºã€‚prefillé˜¶æ®µæ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹çš„ä»»åŠ¡(è€Œdecodeé˜¶æ®µåˆ™æ˜¯ä¸€ä¸ªå†…å­˜å¯†é›†å‹çš„ä»»åŠ¡)å› è€Œæœ‰äº†æ›´å…ˆè¿›çš„åˆ†å¸ƒå¼ Prefill/Decode disaggregation
        * Communication device (éœ€è¦çŸ¥é“è¿™äº›ç»†èŠ‚å—)
            * NVLink: direct GPU-to-GPU communication within a single node. ~900GB/s
            * Infinity Band: High-speed inter-node communication. ~200-400Gb/s
                * for multi-node distributed training/inference wehre nodes need to exchange gradient or activations
            * RDMA: Remote direct memory access bypass OS / zero copy and is the technique that can be implemented
                * over inifinity band which has native rdma support -> high-end cluster like NVIDIA DGX 
                * over layer 2 (ethernet), i.e., RoCEv1 or layer 3 (ip), i.e., RoCEv2 -> needs RDMA-capable NICs on both ends + lossless Ethernet
                * over layer 4 TCP/IP defined by iWARP protocol (software-based)
        * Communication library: `vllm/distributed/device_communicators`å…¶å®è®²çš„æ˜¯ä¸¤ä¸ªè¿›ç¨‹
            * `PyNccl`: communication between NVIDIA hardware for `GPU-to-GPU` tensor transfers (CUDA IPC, NVLink)
            * `shared memory` is the CPU-based inter-process communication (IPC) for synchronization flags/control msgs/metadata exchange
                * within single node, each `GPU worker process` is a standalone OS process (with isolated address space). OS provides a memory segment so that all processes can access for the GPU worker processes to exchange metadata/control signals efficiently
                ```python
                    # Conceptual example in vLLM
                    # Process 1 (GPU 0) and Process 2 (GPU 1) both map the same memory region
                    shm = shared_memory.SharedMemory(name="vllm_metadata", create=True)
                    # Both can read/write without serialization or network overhead
                ```
            * `custom allreduce`: A kernel just for all reduce operation on exchanging/broadcasting tensor data.
                ```
                Before:
                    0 machine: [0]
                    1 machine: [1]
                    2 machine: [2]
                    3 machine: [3]
                After:
                    0 machine: [0, 1, 2, 3]
                    0 machine: [0, 1, 2, 3]
                    0 machine: [0, 1, 2, 3]
                    0 machine: [0, 1, 2, 3]
                ```
            * `torch.distributed`
                * Refer to doc: [torch.distribute.md](./torch.distributed.md)
            * `GroupCoordinator` exists in every worker process. All the group coordinator works together as a decentralized group.
                * It's a wrapper of pytorch ProcessGroup
                * rank: å¯¹è¿›è¡Œåˆ†å¸ƒå¼è®¡ç®—çš„ç¨‹åºçš„ç¼–å·ã€‚å¦‚æœTP=2ï¼ŒPP=4ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦2x4=8ä¸ªworkerè¿›ç¨‹ï¼Œé‚£ä¹ˆæˆ‘ç»™æ¯ä¸ªworkerè¿›ç¨‹ä¸€ä¸ªç¼–å·å°±æ˜¯rank 0-7
                * local_rank: åˆ†é…ç»™å½“å‰worker processçš„æœ¬åœ°å¯ç”¨GPUåœ¨æœ¬æœºç¼–å·
                * rank_in_groupï¼šåˆ†é…ç»™å½“å‰worker processçš„æœ¬åœ°å¯ç”¨GPUåœ¨groupä¸­çš„å…¨å±€ç¼–å·
                * cpu_group: åŒæ­¥å¯æ§æ€§æ¯”gpuæ›´é«˜ï¼ŒåŸºäºcpuçš„é€šä¿¡èƒ½å¤Ÿè®©å‡ ä¸ªworker processåœ¨barrierä¸‹åŒæ­¥ç­‰å¾…
                * device_group: 
            * Algorithm-side
                * TP: how is it implemented within `flash_attn.py`
                * `vllm/model_executor/models/llama.py`
                    * class `LlamaAttention` divides `heads` aross `tp_size`
    * ä¼ ç»Ÿçš„Type of distributed inference: TP / PP/ EP / DP
        * TP: tensor parallel
            * what: å•ä¸ªlayerå†…éƒ¨å¹¶è¡Œè®¡ç®—å¼ é‡è¿ç®—ã€‚æŠŠæ¨¡å‹çš„weightsï¼ŒæŒ‰ç…§GPUçš„ä¸ªæ•°æ‹†æˆç›¸åº”ä»½ï¼ŒåŒæ—¶è¿›è¡Œè®¡ç®—ï¼Œç„¶åå°†æ‰€æœ‰GPUçš„ç»“æœæ±‡æ€»ï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ªlayerçš„è¾“å…¥ã€‚æ˜¯ä¸€ä¸ªæ¨ªå‘åˆ‡layerçš„å¹¶è¡Œè®¡ç®—
            * why: è§£å†³çš„æ˜¯å•å±‚å¤ªå¤§ï¼Œä¸€å¼ å¡çš„å†…å­˜ä¸è¶³ä»¥å®¹çº³æ•´ä¸ªparameters
            * Path for communication data structure: `vllm/distributed/parallel_state.py`
                * 740è¡Œçš„`_TP: GroupCoordinator`æ˜¯vLLMè¿›è¡Œåˆ†å¸ƒå¼é€šä¿¡ï¼Œ932è¡Œ`init_model_parallel_group()`è¿›è¡Œåˆå§‹åŒ–
                * `GroupCoordinator`ä¸ä»…éœ€è¦è€ƒè™‘TPï¼Œè¿˜è¦è€ƒè™‘PP(Pipeline Parallel)ï¼Œä»¥ä¸‹æ˜¯chatgptçš„æè¿°
                    ```
                    å¯¹ä¸€ä¸ª token æ¥è¯´ï¼š
                        Token t:
                        PP stage 0:
                            â”œâ”€â”€ TP rank 0 compute slice
                            â”œâ”€â”€ TP rank 1 compute slice
                            â””â”€â”€ all-reduce
                        send hidden state â†’
                        PP stage 1:
                            â”œâ”€â”€ TP rank 0
                            â”œâ”€â”€ TP rank 1
                            â””â”€â”€ all-reduce
                        ğŸ‘‰ GroupCoordinator å¿…é¡»çŸ¥é“ï¼š
                            * å“ªäº› rank å±äºåŒä¸€ä¸ª TP group
                            * å“ªäº› rank å±äºåŒä¸€ä¸ª PP stage
                            * PP stage ä¹‹é—´çš„é¡ºåº
                            * TP group å†…çš„ collective åŒæ­¥ç‚¹
                        å¦‚æœä¸çŸ¥é“è¿™ä¸¤å±‚ç»“æ„ï¼Œå°±æ— æ³•æ­£ç¡®è°ƒåº¦ token çš„æ‰§è¡Œè·¯å¾„ã€‚ä¸€å¥è¯æ€»ç»“ï¼š
                        * PP æ˜¯åœ¨ layer ç»´åº¦å¹¶è¡Œï¼ˆåˆ‡æ¨¡å‹ç»“æ„ï¼‰ï¼Œ
                        * TP æ˜¯åœ¨ layer å†…éƒ¨æŒ‰å¼ é‡ç»´åº¦å¹¶è¡Œï¼ˆåˆ‡è®¡ç®—ï¼‰ï¼Œ
                        * token æ°¸è¿œæ˜¯â€œç©¿è¿‡â€TP å’Œ PP çš„æ‰§è¡Œå®ä½“ï¼Œè€Œä¸æ˜¯è¢«åˆ‡åˆ†çš„å¯¹è±¡ã€‚
                    ```
        * PP: pipeline parallel
            * what: 
            * why: è§£å†³çš„æ˜¯å±‚æ¬¡å¤ªå¤šçš„é—®é¢˜ï¼ŒåŒºåˆ«äºTPè§£å†³çš„æ˜¯å•å±‚å¤ªå¤§çš„é—®é¢˜ã€‚æ˜¯çºµå‘åˆ‡model blockï¼Œå°†layeræ´¾å‘åˆ°ä¸åŒGPUè¿›è¡Œè®¡ç®—ã€‚
            * cons: cannot improve latency. For a single request, you don't save time in any of its layers of steps.
                * Compared with tensor parallel, tensor parallel improves latency.
                * Used in cheap hardware for easier deployment and higher throughput
            * Algorithm-size:
                * worker in charge of a subset of layers
                    * Path: `vllm/model_executor/models/llama.py`
                    * self.start_layer --> self.end_layer scopes the layers handled by the worker
                    * between workers: communicate `IntermediateTensor`
                    * pipeline parallel
                        * Path: `vllm/worker/model_runner.py`
                        * `get_pp_group()`
        * EP (Expert parallel)
            * why EP: 
                * Mistral/Mixtral/deepseek model: Mixure of Experts (MoE)
                    * Only applies to the linear layers not attention layer
                    * Normal process: all weights participate in computation
                    * MoE: expert as granularity, only a small subset of experts participate the computation, the subset of expert may be different between request.
                        * about only 5 experts (The most possible output gets the weights, the other weights are 0)
                * Place differet experts onto different GPUs --> expert parallel
                * Algorithm:
                    * Context: the matrix is sparse, so we just need to focus on the most significant part of data
                    * Expert parallel:
                        * Shuffle (deepep communication kernel)
                        * Forward
                        * Shuffle back
                * TP is for attention, and EP is for linear layers
                * Shared expert will have high load --> duplicate shared expert ? 
        * DP (data/request parellel)
            * reason: max tp# << ep# needed
            * tp < # attention head
            * tp * dp == ep (ep = 320 process, but tp may be 20 parallelity, then we parallelize data/request)
            * å°±æ˜¯epçš„å¹¶è¡Œåº¦èƒ½åŠ›è¿œå¤§äºtpçš„å¹¶è¡Œåº¦èƒ½åŠ›ï¼Œäºæ˜¯æä¾›åœ¨æ•°æ®å±‚é¢çš„å¹¶è¡Œåº¦ï¼Œæ¥å……åˆ†åˆ©ç”¨epçš„å¹¶è¡Œå¤„ç†èƒ½åŠ›
            * Difficult to implement in paractice: 
                * request padding to avoid deadlock
    * PD Disaggregation
        * KV cache

        * Prefill and Decode
            * Prefill: process input prompts to generate kv cache
                * Time O(n**2), n: #tokens
                * Path: `llama.py` `forward(): qkv_proj() -> qkv.split() -> rotary_emb() -> attn()`
            * Decode: generate output tokens
                * Time O(n)
            * reason
                * çŸ­æ¿ç†è®ºï¼šprioritize prefill
            * problem: prefill step takes time and pauses decoding steps of other requests
                * Solution: 
                    1. PD disaggregation
                        * put prefill to one GPU and the decode to the other GPU
                        * XPXD, XPYD
                            * XPXD: prefill nodes == decode nodes
                            * XPYD: dynamic configure p nodes and d nodes
                    2. Chunked prefill ? 
                        * Treat prefill and decode as the same operation
                        * Design operator that accepts either shape of operations (prefill: matrix multiplication, decode: vector x matrix)
                        * preferred by sheduler that only cares about token x job mappingã€‚æ¯ä¸ªjobè·‘å¤šå°‘ä¸ªtokenã€‚without caring about either prefill or decode
                            * v1 scheduler got simplified a lot because of this
                            * chunk size matters when batch prefill and decode in a single batch
                                * too large chunk size will cause too many prefill work and delay decoding
                                * too small chunk size increases the generation speed but the hardware utilization down
                        * GPU temporary memory buffer proportional to context length, so lots of context tokens would oom
            * Question 1: How to transfer KV cache from p nodes to d nodes?
                * 2 modes: pooling mode and p2p mode
                    * pooling mode: producer/sender -> pool -> consumer/receiver
                        * pros: easier impl decoupling the sender/receiver when relations complex like multiple sender and multiple receiver
                    * p2p mode: sender <-> receiver knows each other
                        * pros: direct transfer to receiver than writing to pool then writing to receiver
                    * implementation
                        * LMCache: implements both
                        * Other impl
                            * MoonCake implements pooling mode
                            * NVIDIA NIXL: implements p2p mode, uses UCX more general data tansfer
                        * All these works as a library for vLLM to use
            * Question 2: How to extract and inject KV cache from/to vLLM?
                * Connector API
                    * Path: `vllm/distributed/kv_connector/simple_connector.py`
                * called by model_runner in `llama.py`
                    * model runner that wraps the underlying model and prepares the needed inputs
                        - Before model forward: `get_kv_transfer_group().recv_kv_caches_and_hidden_states()`
                            * Inject KV cache into vLLM's paged memory
                            * Check if there's kv cache for the input tokens, if there's the update the input token states to reflect the truth before logic flows to scheduler.
                        - model forward
                        - Afer model forard: `get_kv_transfer_group().send_kv_caches_and_hidden_states()`
                            * Extract kv cache from vLLM's paged memory and send it to external 
            * Question 3: When to send request to P node and D node?
                * Original flow: request --> single instance to handle P and D
                * Now flow: request --> P and D instances
                    * design 1: request -> router -> P node -> router -> D node
                    * design 2: request -> D node -> P node -> D node

## vLLM v1
* Why V1?
    * v0 is slow (CPU overhead)
    * v0 code is hard to read and dev
* Scheduler 
    * Path: `vllm/v1/core/sched/scheduler.py`
    * Simplified scheduling: chunked prefill by default
        * Refer to ![Simplified Scheduler](simplified_scheduler.png)
        * `{r: delta_token}` describes the number of extra tokens than last `step`
        * unifies prefill and decode 
            * originial prefill and decode step: ![alt text](scheduler_prefill_decode.png)
            * now chunked prefill step: ![alt text](scheduler_chunked_prefill.png)
            * prefix caching step: ![alt text](scheduler_prefix_caching.png)
* General architecture
    * Scheduler, API Server, EngineCore in separate processes: ![alt text](architecture.png)
        * They use ZMQ msgpack for inter-process communication
        * Refer to this blog [vLLM V1: A Major Upgrade to vLLM's Core Architecture](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html)
    * Scheduler and worker in separate process
        * V0: scheduler and Rank 0 worker co-locate in the same process
        * V1: scheduler in a separate process. So each worker program follows strictly SPMD(same program multiple data): ![alt text](architecture1.png)
        * So for communication with GPU worker, vLLM use Ray?
    * microservic?
        * microservice sacrifices communciation for higher scalability. State is scattered among each microservice than centralized.
        * However, LLM has a very large kv cache and needs a very fast communication among modules. It needs a higher communication than scalability.
        * In the future, we may see the introduction of microservice in the LLM area when service grows too large to handle centrally
        * PD disaggregation shows an early form of microservice, which separates the P service and D service
* Worker
    * Persistent batching
        * We only send the delta of the GPU batch tensors from CPU
    * Piecewise cudagraph
* Attention kernel
    * Simplified configuration
    * Cascade inference

* Multi-modal
    * Embedding as the new KV cache reference
    * KV cache management (incoming)
        * Hybrid memory allocator

## Misc
* ç³»ç»Ÿä¸­å¤„ç†å¤æ‚é—®é¢˜çš„æ–¹å¼å°±æ˜¯æŠ½è±¡ï¼Œå°±æ˜¯å¯è¿ç§»åˆ°å…·ä½“é—®é¢˜çš„æ›´é«˜å±‚æœ¬è´¨è§„å¾‹
* ä¸€ä¸ªæˆåŠŸçš„äººç”Ÿé˜¶æ¢¯å°±æ˜¯ä»è¿™ä¸€ä¸ªé˜¶æ®µçš„å¥½çš„projectè¿›å…¥åˆ°ä¸‹ä¸€ä¸ªå¥½çš„project
* Microsoft foundry vs vLLM vs DeepSpeed
    ```
        Foundry: äº§å“/å•†ä¸šå±‚
        â†“
        vLLM/TRT: å¹³å°è°ƒåº¦å±‚
        â†“
        DeepSpeed/ZeRO: æ¨¡å‹/ç®—æ³•å±‚
    ```
    * Foundryï¼šâ€œèƒ½ä¸èƒ½å¯¹å¤–å–â€
    * vLLMï¼šâ€œç®—å¾—æ˜¯å¦é«˜æ•ˆâ€
    * DeepSpeedï¼šâ€œèƒ½ä¸èƒ½ç®—â€